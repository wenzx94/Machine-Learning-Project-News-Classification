{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Classification \n",
    "\n",
    "In this project, We used 6500 pieces of news from [Mashable](http://mashable.com/).\n",
    "Preprocessing was applied to stokenlize and vectorize the dataset. Multiple machine learning models were used. For each model, we recorded the runing time and the accuracy and compared the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the news data\n",
    "This dataset contains 6500 pieces of news, each contains url, label and article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>\n",
       "      <td>business</td>\n",
       "      <td>The Associated Press is the latest news organi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>\n",
       "      <td>business</td>\n",
       "      <td>It looks like 2012 was a pretty good year for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>When it comes to college football, NASA astron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n",
       "      <td>tech</td>\n",
       "      <td>LAS VEGAS — Sharing photos and videos on your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/01/07/beewi-smart-toys/</td>\n",
       "      <td>tech</td>\n",
       "      <td>LAS VEGAS — RC toys have traded in their bulky...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0              1  \\\n",
       "0  http://mashable.com/2013/01/07/ap-samsung-spon...       business   \n",
       "1  http://mashable.com/2013/01/07/apple-40-billio...       business   \n",
       "2  http://mashable.com/2013/01/07/astronaut-notre...  entertainment   \n",
       "3   http://mashable.com/2013/01/07/att-u-verse-apps/           tech   \n",
       "4   http://mashable.com/2013/01/07/beewi-smart-toys/           tech   \n",
       "\n",
       "                                                   2  \n",
       "0  The Associated Press is the latest news organi...  \n",
       "1  It looks like 2012 was a pretty good year for ...  \n",
       "2  When it comes to college football, NASA astron...  \n",
       "3  LAS VEGAS — Sharing photos and videos on your ...  \n",
       "4  LAS VEGAS — RC toys have traded in their bulky...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('articles.csv', header=None)\n",
    "df = df.dropna(axis=0,how='any')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6500, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = np.array(df)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "In our project, we used TF-IDF features for training model. However, punctuations, numbers, some stop-words, tense of verbs (For example, do and did) would affect the result of TF-IDF features. In order to address this problem, we preprocessed articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset[:, 1]\n",
    "raw_articles = dataset[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before preprocessing, we use the first piece of news as example to show how the articles look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Associated Press is the latest news organization to experiment with trying to make money from Twitter by using its feed to advertise for other companies. The AP announced Monday that it will share sponsored tweets from Samsung throughout this week for the International CES taking place in Las Vegas. The news service will let Samsung post two tweets per day to the AP's Twitter account, which has more than 1.5 million users, and each of these tweets will be labeled \"SPONSORED TWEETS.\"This marks the first time that the AP has sold advertising on its Twitter feed, and the company says it spent months developing guidelines to pave the way for this and other new media business models. For this particular promotion, Samsung will provide the sponsored tweets and non-editorial staff at the AP will handle the publishing side. In this way, the company hopes to maintain a clear dividing line between its editorial and advertising operations on Twitter.\"We are thrilled to be taking this next step in social media,\" said Lou Ferrara, the AP managing editor overseeing its social media efforts, in a statement. \"As an industry, we must be looking for new ways to develop revenues while providing good experiences for advertisers and consumers. At the same time, advertisers and audiences expect AP to do that without compromising its core mission of breaking news.\" Other publishers have dabbled in Twitter ads, including The Atlantic, National Journal, The Times-Picayune and BreakingNews.com.Image courtesy of Flickr, nan palmero\n"
     ]
    }
   ],
   "source": [
    "print(raw_articles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nltk package is applied to preprocess data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "clean_articles = []\n",
    "for text in raw_articles:\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in words]\n",
    "    string = \" \".join(stemmed)\n",
    "    clean_articles.append(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Articles are preprocessed into format below. numbers, punctuations, case and tense are remove from artciles. This would make it hard to read by human, but it would make the data cleaner. \n",
    "We displayed the first article as we showed above after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "associ press latest news organ experi tri make money twitter use feed advertis compani ap announc monday share sponsor tweet samsung throughout week intern ce take place la vega news servic let samsung post two tweet per day ap twitter account million user tweet label sponsor tweet mark first time ap sold advertis twitter feed compani say spent month develop guidelin pave way new media busi model particular promot samsung provid sponsor tweet staff ap handl publish side way compani hope maintain clear divid line editori advertis oper twitter thrill take next step social media said lou ferrara ap manag editor overse social media effort statement industri must look new way develop revenu provid good experi advertis consum time advertis audienc expect ap without compromis core mission break news publish dabbl twitter ad includ atlant nation journal courtesi flickr nan palmero\n"
     ]
    }
   ],
   "source": [
    "print(clean_articles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Vectorizing\n",
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)(term frequency–inverse document frequency) is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. This provide us a way to classify articles, based on the occurance frequency of import words in articles.\n",
    "\n",
    "In this process, we tokenize all dataset, extract top 12000 words from dataset. Then those top 12000 words were converted to tf-idf format, and vectorize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=12000,\n",
    "                            min_df=2, use_idf=True, lowercase=True)\n",
    "X = vectorizer.fit_transform(clean_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we would use y to present labels and convert it to numbers which can be used in neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business' 'business' 'entertainment' 'tech' 'tech' 'lifestyle' 'tech'\n",
      " 'tech' 'world' 'world']\n",
      "[0 0 1 2 2 3 2 2 4 4]\n"
     ]
    }
   ],
   "source": [
    "y = labels\n",
    "print(y[:10])\n",
    "numy = pd.factorize(y)[0]\n",
    "print(numy[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-logistic Regression with Cross Validation\n",
    "Multi-logistic Regression is the basic way to classfiy data set. In this process, we used this method to train our data and used cross validation to verify.\n",
    "Notice that the implement of cross validation is referenced from \"Breast Cancer Diagnosis via Logistic Regression\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "multilogreg = linear_model.LogisticRegression(C=1e4, solver = 'newton-cg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Time of Group 1 : 3.030 seconds\n",
      "Running Time of Group 2 : 2.873 seconds\n",
      "Running Time of Group 3 : 2.945 seconds\n",
      "Running Time of Group 4 : 3.130 seconds\n",
      "Total running time = 11.979 seconds\n",
      "================summary==================\n",
      "Precision = 0.7061, SE=0.0105\n",
      "Recall =    0.7085, SE=0.0095\n",
      "f1 =        0.7057, SE=0.0100\n",
      "Accuracy =  0.7085, SE=0.0095\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "nfold = 4\n",
    "kf = KFold(n_splits=nfold)\n",
    "prec = []\n",
    "rec = []\n",
    "f1 = []\n",
    "acc = []\n",
    "t = []\n",
    "i = 1\n",
    "for train, test in kf.split(X):  \n",
    "    start = time.time()\n",
    "    # Get training and test data\n",
    "    Xtr = X[train,:]\n",
    "    ytr = numy[train]\n",
    "    Xts = X[test,:]\n",
    "    yts = numy[test]\n",
    "    \n",
    "    # Fit a model\n",
    "    multilogreg.fit(Xtr, ytr)\n",
    "    yhat = multilogreg.predict(Xts)\n",
    "    \n",
    "    # Measure performance\n",
    "    preci,reci,f1i,_= precision_recall_fscore_support(yts,yhat,average = 'weighted') \n",
    "    prec.append(preci)\n",
    "    rec.append(reci)\n",
    "    f1.append(f1i)\n",
    "    acci = np.mean(yhat == yts)\n",
    "    acc.append(acci)\n",
    "    end = time.time()\n",
    "    t.append(end-start)\n",
    "    print('Running Time of Group %d : %.3f seconds' %(i, end-start))\n",
    "    i = i+1\n",
    "\n",
    "# Take average values of the metrics\n",
    "precm = np.mean(prec)\n",
    "recm = np.mean(rec)\n",
    "f1m = np.mean(f1)\n",
    "accm= np.mean(acc)\n",
    "\n",
    "# Compute the standard errors\n",
    "prec_se = np.std(prec)/np.sqrt(nfold-1)\n",
    "rec_se = np.std(rec)/np.sqrt(nfold-1)\n",
    "f1_se = np.std(f1)/np.sqrt(nfold-1)\n",
    "acc_se = np.std(acc)/np.sqrt(nfold-1)\n",
    "total_time = sum(t)\n",
    "print('Total running time = %.3f seconds' % total_time)\n",
    "print('================summary==================')\n",
    "print('Precision = {0:.4f}, SE={1:.4f}'.format(precm,prec_se))\n",
    "print('Recall =    {0:.4f}, SE={1:.4f}'.format(recm, rec_se))\n",
    "print('f1 =        {0:.4f}, SE={1:.4f}'.format(f1m, f1_se))\n",
    "print('Accuracy =  {0:.4f}, SE={1:.4f}'.format(accm, acc_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine with Cross Validation\n",
    "\n",
    "Support Vector Machine is also a useful model classifing data set. K-fold was applied to cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svc = svm.SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Time of Group 1 : 29.168 seconds\n",
      "Running Time of Group 2 : 27.713 seconds\n",
      "Running Time of Group 3 : 30.667 seconds\n",
      "Running Time of Group 4 : 29.773 seconds\n",
      "Total running time = 117.321 seconds\n",
      "================summary==================\n",
      "Precision = 0.7397, SE=0.0093\n",
      "Recall =    0.7398, SE=0.0086\n",
      "f1 =        0.7361, SE=0.0095\n",
      "Accuracy =  0.7398, SE=0.0086\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "t = []\n",
    "nfold = 4\n",
    "kf = KFold(n_splits=nfold)\n",
    "prec = []\n",
    "rec = []\n",
    "f1 = []\n",
    "acc = []\n",
    "i = 1\n",
    "for train, test in kf.split(X): \n",
    "    start = time.time()\n",
    "    # Get training and test data\n",
    "    Xtr = X[train,:]\n",
    "    ytr = numy[train]\n",
    "    Xts = X[test,:]\n",
    "    yts = numy[test]\n",
    "    \n",
    "    # Fit a model\n",
    "    svc.fit(Xtr, ytr)\n",
    "    yhat = svc.predict(Xts)\n",
    "    \n",
    "    # Measure performance\n",
    "    preci,reci,f1i,_= precision_recall_fscore_support(yts,yhat,average = 'weighted') \n",
    "    prec.append(preci)\n",
    "    rec.append(reci)\n",
    "    f1.append(f1i)\n",
    "    acci = np.mean(yhat == yts)\n",
    "    acc.append(acci)\n",
    "    end = time.time()\n",
    "    t.append(end-start)\n",
    "    print('Running Time of Group %d : %.3f seconds' %(i, end-start))\n",
    "    i = i+1\n",
    "\n",
    "# Take average values of the metrics\n",
    "precm = np.mean(prec)\n",
    "recm = np.mean(rec)\n",
    "f1m = np.mean(f1)\n",
    "accm= np.mean(acc)\n",
    "\n",
    "# Compute the standard errors\n",
    "prec_se = np.std(prec)/np.sqrt(nfold-1)\n",
    "rec_se = np.std(rec)/np.sqrt(nfold-1)\n",
    "f1_se = np.std(f1)/np.sqrt(nfold-1)\n",
    "acc_se = np.std(acc)/np.sqrt(nfold-1)\n",
    "total_time = sum(t)\n",
    "print('Total running time = %.3f seconds' % total_time)\n",
    "print('================summary==================')\n",
    "print('Precision = {0:.4f}, SE={1:.4f}'.format(precm,prec_se))\n",
    "print('Recall =    {0:.4f}, SE={1:.4f}'.format(recm, rec_se))\n",
    "print('f1 =        {0:.4f}, SE={1:.4f}'.format(f1m, f1_se))\n",
    "print('Accuracy =  {0:.4f}, SE={1:.4f}'.format(accm, acc_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Time of Group 1 : 0.026 seconds\n",
      "Running Time of Group 2 : 0.020 seconds\n",
      "Running Time of Group 3 : 0.019 seconds\n",
      "Running Time of Group 4 : 0.018 seconds\n",
      "Total running time = 0.083 seconds\n",
      "================summary==================\n",
      "Precision = 0.7070, SE=0.0069\n",
      "Recall =    0.7102, SE=0.0062\n",
      "f1 =        0.7057, SE=0.0062\n",
      "Accuracy =  0.7102, SE=0.0062\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "t = []\n",
    "nfold = 4\n",
    "kf = KFold(n_splits=nfold)\n",
    "prec = []\n",
    "rec = []\n",
    "f1 = []\n",
    "acc = []\n",
    "i = 1\n",
    "for train, test in kf.split(X): \n",
    "    start = time.time()\n",
    "    # Get training and test data\n",
    "    Xtr = X[train,:]\n",
    "    ytr = numy[train]\n",
    "    Xts = X[test,:]\n",
    "    yts = numy[test]\n",
    "    \n",
    "    # Fit a model\n",
    "    clf = MultinomialNB(alpha = 0.01)\n",
    "    clf.fit(Xtr, ytr)\n",
    "    yhat = clf.predict(Xts)\n",
    "    \n",
    "    # Measure performance\n",
    "    preci,reci,f1i,_= precision_recall_fscore_support(yts,yhat,average = 'weighted') \n",
    "    prec.append(preci)\n",
    "    rec.append(reci)\n",
    "    f1.append(f1i)\n",
    "    acci = np.mean(yhat == yts)\n",
    "    acc.append(acci)\n",
    "    end = time.time()\n",
    "    t.append(end-start)\n",
    "    print('Running Time of Group %d : %.3f seconds' %(i, end-start))\n",
    "    i = i+1\n",
    "\n",
    "# Take average values of the metrics\n",
    "precm = np.mean(prec)\n",
    "recm = np.mean(rec)\n",
    "f1m = np.mean(f1)\n",
    "accm= np.mean(acc)\n",
    "\n",
    "# Compute the standard errors\n",
    "prec_se = np.std(prec)/np.sqrt(nfold-1)\n",
    "rec_se = np.std(rec)/np.sqrt(nfold-1)\n",
    "f1_se = np.std(f1)/np.sqrt(nfold-1)\n",
    "acc_se = np.std(acc)/np.sqrt(nfold-1)\n",
    "total_time = sum(t)\n",
    "print('Total running time = %.3f seconds' % total_time)\n",
    "print('================summary==================')\n",
    "print('Precision = {0:.4f}, SE={1:.4f}'.format(precm,prec_se))\n",
    "print('Recall =    {0:.4f}, SE={1:.4f}'.format(recm, rec_se))\n",
    "print('f1 =        {0:.4f}, SE={1:.4f}'.format(f1m, f1_se))\n",
    "print('Accuracy =  {0:.4f}, SE={1:.4f}'.format(accm, acc_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation\n",
    "We shuffle the 6500 data by pertation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = np.random.permutation(6500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group the data\n",
    "We separate the data into 2 groups for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = X[permutation[:6000]].todense()\n",
    "Xts = X[permutation[6000:6500]].todense()\n",
    "ytr = numy[permutation[:6000]]\n",
    "yts = numy[permutation[6000:6500]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential \n",
    "from keras.layers import Dense, Activation\n",
    "import keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a network. The features are:\n",
    "* We have one hidden layer with nh=500 units.\n",
    "* One output layer with nout=6 units, one for each of the 6 possible classes\n",
    "* The output activation is softmax, which is used for multi-class targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nin = X.shape[1]  # dimension of input data\n",
    "nh = 500     # number of hidden units\n",
    "nout = 6    # number of outputs = 10 since there are 10 classes\n",
    "model = Sequential()\n",
    "model.add(Dense(nh, input_shape=(nin,), activation='relu', name='hidden'))\n",
    "model.add(Dense(nout, activation='softmax', name='output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden (Dense)               (None, 500)               6000500   \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 6)                 3006      \n",
      "=================================================================\n",
      "Total params: 6,003,506\n",
      "Trainable params: 6,003,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keep track of the loss history and validation accuracy\n",
    "This part is referenced from \"Lab 7: Neural Networks for Music Classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # TODO:  Create two empty lists, self.loss and self.val_acc\n",
    "        self.loss = list()\n",
    "        self.val_acc = list()\n",
    " \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        # TODO:  This is called at the end of each batch.  \n",
    "        # Add the loss in logs.get('loss') to the loss list\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        # TODO:  This is called at the end of each epoch.  \n",
    "        # Add the test accuracy in logs.get('val_acc') to the val_acc list\n",
    "        self.val_acc.append(logs.get('val_acc'))\n",
    "\n",
    "# Create an instance of the history callback\n",
    "history_cb = LossHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an optimizer and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "opt = optimizers.Adam(lr=0.001) # beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 500 samples\n",
      "Epoch 1/5\n",
      "6000/6000 [==============================] - 5s 910us/step - loss: 1.2936 - acc: 0.6275 - val_loss: 0.8787 - val_acc: 0.7360\n",
      "Epoch 2/5\n",
      "6000/6000 [==============================] - 5s 885us/step - loss: 0.5682 - acc: 0.8338 - val_loss: 0.7081 - val_acc: 0.7780\n",
      "Epoch 3/5\n",
      "6000/6000 [==============================] - 5s 882us/step - loss: 0.3080 - acc: 0.9198 - val_loss: 0.6956 - val_acc: 0.7880\n",
      "Epoch 4/5\n",
      "6000/6000 [==============================] - 5s 889us/step - loss: 0.1696 - acc: 0.9692 - val_loss: 0.7158 - val_acc: 0.7800\n",
      "Epoch 5/5\n",
      "6000/6000 [==============================] - 5s 884us/step - loss: 0.0964 - acc: 0.9882 - val_loss: 0.7438 - val_acc: 0.7700\n",
      "Total running time = 26.912 seconds\n"
     ]
    }
   ],
   "source": [
    "batch_size = 500\n",
    "start = time.time()\n",
    "model.fit(Xtr, ytr, epochs=5, batch_size=100, validation_data=(Xts,yts), callbacks = [history_cb])\n",
    "end = time.time()\n",
    "print('Total running time = %.3f seconds' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
